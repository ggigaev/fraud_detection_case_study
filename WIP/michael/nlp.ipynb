{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF as NMF_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMF(object):\n",
    "    '''\n",
    "    A Non-Negative Matrix Factorization (NMF) model using the Alternating Least\n",
    "    Squares (ALS) algorithm.\n",
    "    This class represents an NMF model, which is a useful unsupervised data\n",
    "    mining tool; e.g. for finding latent topics in a text corpus such as NYT\n",
    "    articles.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, k, max_iters=50, alpha=0.5, eps=1e-6):\n",
    "        '''\n",
    "        Constructs an NMF object which will mine for `k` latent topics.\n",
    "        The `max_iters` parameter gives the maximum number of ALS iterations\n",
    "        to perform. The `alpha` parameter is the learning rate, it should range\n",
    "        in (0.0, 1.0]. `alpha` near 0.0 causes the model parameters to be\n",
    "        learned slowly over many many ALS iterations, while an alpha near 1.0\n",
    "        causes model parameters to be fit quickly over very few ALS iterations.\n",
    "        '''\n",
    "        self.k = k\n",
    "        self.max_iters = max_iters\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "\n",
    "    def _fit_one(self, V):\n",
    "        '''\n",
    "        Do one ALS iteration. This method updates self.H and self.W\n",
    "        and returns None.\n",
    "        '''\n",
    "        # Fit H while holding W constant:\n",
    "        H_new = np.linalg.lstsq(self.W, V)[0].clip(min=self.eps)\n",
    "        self.H = self.H * (1.0 - self.alpha) + H_new * self.alpha\n",
    "\n",
    "        # Fit W while holding H constant:\n",
    "        W_new = np.linalg.lstsq(self.H.T, V.T)[0].T.clip(min=self.eps)\n",
    "        self.W = self.W * (1.0 - self.alpha) + W_new * self.alpha\n",
    "\n",
    "    def fit(self, V, verbose = False):\n",
    "        '''\n",
    "        Do many ALS iterations to factorize `V` into matrices `W` and `H`.\n",
    "        Let `V` be a matrix (`n` x `m`) where each row is an observation\n",
    "        and each column is a feature. `V` will be factorized into a the matrix\n",
    "        `W` (`n` x `k`) and the matrix `H` (`k` x `m`) such that `WH` approximates\n",
    "        `V`.\n",
    "        This method returns the tuple (W, H); `W` and `H` are each ndarrays.\n",
    "        '''\n",
    "        n, m = V.shape\n",
    "        self.W = np.random.uniform(low=0.0, high=1.0 / self.k, size=(n, self.k))\n",
    "        self.H = np.random.uniform(low=0.0, high=1.0 / self.k, size=(self.k, m))\n",
    "        for i in range(self.max_iters):\n",
    "            if verbose:\n",
    "                print('iter', i, ': reconstruction error:', self.reconstruction_error(V))\n",
    "            self._fit_one(V)\n",
    "        if verbose:\n",
    "            print('FINAL reconstruction error:', self.reconstruction_error(V), '\\n')\n",
    "        return self.W, self.H\n",
    "\n",
    "    def reconstruction_error(self, V):\n",
    "        '''\n",
    "        Compute and return the reconstruction error of `V` as the\n",
    "        matrix L2-norm of the residual matrix.\n",
    "        See: https://en.wikipedia.org/wiki/Matrix_norm\n",
    "        See: https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html\n",
    "        '''\n",
    "        return np.linalg.norm(V - self.W.dot(self.H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14337, 44)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('data.json')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df[df.dtypes[df.dtypes==object].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = df.drop(text.columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = nums[['approx_payout_date','event_created','event_end','event_published','event_start','user_created']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = nums.drop(dates.columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.abs(1 - text.acct_type.str.contains('fraud'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(text.previous_payouts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates['approx_payout_date']=dates['approx_payout_date'].astype(\"datetime64[s]\")\n",
    "dates['event_created']=dates['event_created'].astype(\"datetime64[s]\")\n",
    "dates['event_end']=dates['event_end'].astype(\"datetime64[s]\")\n",
    "dates['event_published']=dates['event_published'].astype(\"datetime64[s]\")\n",
    "dates['event_start']=dates['event_start'].astype(\"datetime64[s]\")\n",
    "dates['user_created']=dates['user_created'].astype(\"datetime64[s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_vectorizer(contents, use_tfidf=True, use_stemmer=False, max_features=None):\n",
    "    '''\n",
    "    Build and return a **callable** for transforming text documents to vectors,\n",
    "    as well as a vocabulary to map document-vector indices to words from the\n",
    "    corpus. The vectorizer will be trained from the text documents in the\n",
    "    `contents` argument. If `use_tfidf` is True, then the vectorizer will use\n",
    "    the Tf-Idf algorithm, otherwise a Bag-of-Words vectorizer will be used.\n",
    "    The text will be tokenized by words, and each word will be stemmed iff\n",
    "    `use_stemmer` is True. If `max_features` is not None, then the vocabulary\n",
    "    will be limited to the `max_features` most common words in the corpus.\n",
    "    '''\n",
    "    Vectorizer = TfidfVectorizer if use_tfidf else CountVectorizer\n",
    "    tokenizer = RegexpTokenizer(r\"[\\w']+\")\n",
    "    stem = PorterStemmer().stem if use_stemmer else (lambda x: x)\n",
    "    stop_set = set(stopwords.words('english'))\n",
    "\n",
    "    # Closure over the tokenizer et al.\n",
    "    def tokenize(text):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        stems = [stem(token) for token in tokens if token not in stop_set]\n",
    "        return stems\n",
    "\n",
    "    vectorizer_model = Vectorizer(tokenizer=tokenize, max_features=max_features)\n",
    "    vectorizer_model.fit(contents)\n",
    "    vocabulary = np.array(vectorizer_model.get_feature_names())\n",
    "\n",
    "    # Closure over the vectorizer_model's transform method.\n",
    "    def vectorizer(X):\n",
    "        return vectorizer_model.transform(X).toarray()\n",
    "\n",
    "    return vectorizer, vocabulary\n",
    "\n",
    "\n",
    "def softmax(v, temperature=1.0):\n",
    "    '''\n",
    "    A heuristic to convert arbitrary positive values into probabilities.\n",
    "    See: https://en.wikipedia.org/wiki/Softmax_function\n",
    "    '''\n",
    "    expv = np.exp(v / temperature)\n",
    "    s = np.sum(expv)\n",
    "    return expv / s\n",
    "\n",
    "\n",
    "def hand_label_topics(H, vocabulary):\n",
    "    '''\n",
    "    Print the most influential words of each latent topic, and prompt the user\n",
    "    to label each topic. The user should use their humanness to figure out what\n",
    "    each latent topic is capturing.\n",
    "    '''\n",
    "    hand_labels = []\n",
    "    for i, row in enumerate(H):\n",
    "        top_five = np.argsort(row)[::-1][:20]\n",
    "        print('topic', i)\n",
    "        print('-->', ' '.join(vocabulary[top_five]))\n",
    "        label = input('please label this topic: ')\n",
    "        hand_labels.append(label)\n",
    "        print(label)\n",
    "    return hand_labels\n",
    "\n",
    "\n",
    "def analyze_article(article_index, contents, web_urls, W, hand_labels):\n",
    "    '''\n",
    "    Print an analysis of a single NYT articles, including the article text\n",
    "    and a summary of which topics it represents. The topics are identified\n",
    "    via the hand-labels which were assigned by the user.\n",
    "    '''\n",
    "    print(web_urls[article_index])\n",
    "    print(contents[article_index])\n",
    "    probs = softmax(W[article_index], temperature=0.01)\n",
    "    for prob, label in zip(probs, hand_labels):\n",
    "        print ('--> {:.2f}% {}'.format(prob * 100, label))\n",
    "    \n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Run the unsupervised analysis of the NYT corpus, using NMF to find latent\n",
    "    topics. The user will be prompted to label each latent topic, then a few\n",
    "    articles will be analyzed to see which topics they contain.\n",
    "    '''\n",
    "    # Load the corpus.\n",
    "    #df = pd.read_pickle(\"data/articles.pkl\")\n",
    "    #contents = df.content\n",
    "    #web_urls = df.web_url\n",
    "    contents = text.description\n",
    "    web_urls = text.name \n",
    "\n",
    "    # Build our text-to-vector vectorizer, then vectorize our corpus.\n",
    "    vectorizer, vocabulary = build_text_vectorizer(contents,\n",
    "                                 use_tfidf=True,\n",
    "                                 use_stemmer=True,\n",
    "                                 max_features=5000)\n",
    "    X = vectorizer(contents)\n",
    "\n",
    "    # We'd like to see consistent results, so set the seed.\n",
    "    np.random.seed(12345)\n",
    "\n",
    "    # Find latent topics using our NMF model.\n",
    "    factorizer = NMF(k=100, max_iters=35, alpha=0.5)\n",
    "    W, H = factorizer.fit(X, verbose=True)\n",
    "\n",
    "    # Label topics and analyze a few NYT articles.\n",
    "    # Btw, if you haven't modified anything, the seven topics which should\n",
    "    # pop out are:  (you should type these as the labels when prompted)\n",
    "    #  1. \"football\",\n",
    "    #  2. \"arts\",\n",
    "    #  3. \"baseball\",\n",
    "    #  4. \"world news (middle eastern?)\",\n",
    "    #  5. \"politics\",\n",
    "    #  6. \"world news (war?)\",\n",
    "    #  7. \"economics\"\n",
    "    hand_labels = hand_label_topics(H, vocabulary)\n",
    "    rand_articles = np.random.choice(range(len(W)), 15)\n",
    "    for i in rand_articles:\n",
    "        analyze_article(i, contents, web_urls, W, hand_labels)\n",
    "\n",
    "    # Do it all again, this time using scikit-learn.\n",
    "    nmf = NMF_sklearn(n_components=100, max_iter=100, random_state=12345, alpha=0.0)\n",
    "    W = nmf.fit_transform(X)\n",
    "    H = nmf.components_\n",
    "    print('reconstruction error:', nmf.reconstruction_err_)\n",
    "    hand_labels = hand_label_topics(H, vocabulary)\n",
    "    for i in rand_articles:\n",
    "        analyze_article(i, contents, web_urls, W, hand_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
